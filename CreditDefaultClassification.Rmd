---
title: "Credit Default Classification"
author: "Ankit Raina"
date: "March 21, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading packages
```{r}
require(tidyverse)
require(dplyr)
require(ggplot2)
require(scales)
require(caret)
require(MASS)
require(mlbench)
require(FSelector)
require(e1071)
require(randomForest)
require(pROC)
require(xgboost)
require(LiblineaR)
require(rpartScore)
require(naivebayes)
require(RRF)
require(DMwR)
require(InformationValue)
require(corrplot)
require(MLmetrics)
```

## Options
```{r}
options(scipen=999)         # Avoid exponential notations
options(max.print=999999) 
```

## Helper Functions
```{r}
# Function to create histogram
create_histogram <- function(data, col, col_name, num_bins){
  ggplot(data, aes(x=col)) +
    geom_histogram(bins = num_bins, fill = "purple", col = "blue", alpha = 0.3) +
    labs(title=cat("Distribution of", col_name), x=col_name, y="Count") +
    geom_density(alpha=.2, fill="#FF6666") +
    geom_vline(aes(xintercept=mean(col)),
               color="blue", linetype="dashed", size=1) +
    geom_vline(aes(xintercept=median(col)),
               color="red", linetype="dashed", size=1)  +
    scale_x_continuous(breaks = seq(min(0), max(90), by = 5), na.value = TRUE)
}


# Function to create bar plot
create_bar_plot <- function(data, col, col_name){
  ggplot(data, aes(x=col, fill = col)) + 
    geom_bar(aes(y = (..count..)/sum(..count..))) +
    labs(title= cat("Distribution by", col_name), 
         x=col_name,
         fill=col_name,
         y="Percentage")
}


# Function to create stacked bar plot
create_stacked_bar_plot <- function(data, col1, col1_name, col2, col2_name){
  ggplot(data) + 
    geom_bar(aes(y =(..count..)/sum(..count..), x=col1, fill=col2)) +
    labs(title= cat("Distribution by", col1_name, "and", col2_name), 
         x=col1_name,
         fill=col2_name,
         y="Percentage")
}


# Function to create feature plot
create_feature_plot <- function(col_x, col_y, plot_type){
  featurePlot(x = col_x,                       
              y = col_y,    
              plot = plot_type,                         
              scales=list(x=list(relation="free"), y=list(relation="free")), auto.key=T
  )
}

# Function to calculate classification accuracy
calc_acc = function(actual, predicted) {
  mean(actual == predicted)
}

# Function to get the best model
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

# Function to plot ROC
plot_roc <- function(probs, target){
  roc_ROCR <- performance(prediction(probs, target), measure = "tpr", x.measure = "fpr")
  plot(roc_ROCR, main = "ROC curve", colorize = T)
  abline(a = 0, b = 1)
}

# Function to calculate KS statistic
KS <- function(pred, depvar){
  require("ROCR")
  p   <- prediction(as.numeric(pred),depvar)
  perf <- performance(p, "tpr", "fpr")
  ks <- max(attr(perf, "y.values")[[1]] - (attr(perf, "x.values")[[1]]))
  
  return(ks)
}

# Function to evaluate confusion matrix
evaluate_confusion_matrix <- function(data, model, threshold){
  require(caret)
  
  if(missing(threshold)){
    class <- predict(model, newdata=data)
  }
  else{
    probs <- predict(model, newdata=data, type = 'prob')
    class <- as.factor(ifelse(probs[, "Yes"] > threshold, "Yes", "No"))
  }
  
  caret::confusionMatrix(data = class, reference = data$default.payment.next.month, positive = "Yes", mode = "prec_recall")
}

# Function to evaluate Receiver Operating Characteristics (ROC) Curve
evaluate_roc_auc <- function(data, model){
  require("ROCR")
  require(caret)
  
  probs <- predict(model, newdata=data, type = 'prob')
  pred_ROCR <- prediction(probs[, "Yes"], data$default.payment.next.month)
  roc_ROCR <- performance(pred_ROCR, measure = "tpr", x.measure = "fpr")
  plot(roc_ROCR, main = "ROC curve", colorize = T)
  abline(a = 0, b = 1)
  
  auc_ROCR <- performance(pred_ROCR, measure = "auc")
  auc_ROCR <- auc_ROCR@y.values[[1]]
  
  cat("AUC: ", auc_ROCR)
}

# Function to evaluate Kolmogorov Smirnov statstic
evaluate_KS_statistic <- function(data, model, showChart){
  ks_statistic <- ks_stat(as.numeric(data$default.payment.next.month)-1, as.numeric(predict(model, newdata=data))-1, returnKSTable = F)
  
  cat("KS Statistic: ", ks_statistic)
  
  ks_stat(as.numeric(data$default.payment.next.month)-1, as.numeric(predict(model, newdata=data))-1, returnKSTable = showChart)
  ks_plot(as.numeric(data$default.payment.next.month)-1, as.numeric(predict(model, newdata=data))-1)
  
  return(ks_statistic)
}

# Function to calculate F1-score
f1 <- function(data, lev = NULL, model = NULL) {
  require(MLmetrics)
  
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = "Yes")
  c(F1 = f1_val)
}
```

## Reading the dataset
```{r}
credit_card_data <- read.csv("UCI_Credit_Card.csv", header = T)
```

## Data Cleaning

### Structure of the dataset
```{r}
str(credit_card_data)
```

### Glance at the data
```{r}
head(credit_card_data)
```

### Summary of the data
```{r}
summary(credit_card_data)
```

### Determining observation with all variables missing (all null values)
```{r}
credit_card_data[!complete.cases(credit_card_data), ]
```

### Determining which variables have missing values
```{r}
sapply(credit_card_data, function(x) sum(is.na(x)))
```

#### We can see that we have no missing values


### Dropping the ID Column
```{r}
credit_card_data$ID <- NULL
```

### Converting categorical variables to factors

#### Converting variable SEX having values (1,2) to (Male, Female)
```{r}
credit_card_data$SEX = as.factor(credit_card_data$SEX)
levels(credit_card_data$SEX) <- c("Male","Female")
```

#### Converting variable EDUCATION having values (0,1,2,3,4,5,6) to (Unknown, Graduate school, University, High school, Others, Unknown, Unknown)
```{r}
credit_card_data$EDUCATION = as.factor(credit_card_data$EDUCATION)
levels(credit_card_data$EDUCATION) <- c("Unknown", "Graduate School", "University", "High school", "Others", "Unknown", "Unknown")
```

#### Converting variable MARRIAGE having values (0,1,2,3) to (Unknown, Married, Single, Others)
```{r}
credit_card_data$MARRIAGE <- as.factor(credit_card_data$MARRIAGE)
levels(credit_card_data$MARRIAGE) <- c("Unknown" , "Married" , "Single" ,"Others")
```

#### Converting variable default.payment.next.month having values (0,1) to (No, Yes)
```{r}
credit_card_data$default.payment.next.month <- as.factor(credit_card_data$default.payment.next.month)
levels(credit_card_data$default.payment.next.month) <- c("No" , "Yes")
```

#### Converting repayment status variables to factors
```{r}
credit_card_data$PAY_0 <- as.factor(credit_card_data$PAY_0)
credit_card_data$PAY_2 <- as.factor(credit_card_data$PAY_2)
credit_card_data$PAY_3 <- as.factor(credit_card_data$PAY_3)
credit_card_data$PAY_4 <- as.factor(credit_card_data$PAY_4)
credit_card_data$PAY_5 <- as.factor(credit_card_data$PAY_5)
credit_card_data$PAY_6 <- as.factor(credit_card_data$PAY_6)
```

## Data Exploration and Feauture Selection
```{r}
factor_var_data <- credit_card_data %>% 
  Filter(f = is.factor)

numeric_var_data <- credit_card_data %>% 
  Filter(f = is.numeric)

```

### Creating histograms for numeric variables LIMIT_BAL and AGE

#### LIMIT_BAL
```{r}
create_histogram(credit_card_data, credit_card_data$LIMIT_BAL, "LIMIT_BAL", 20)
```

The distribution is slightly skewed to the right, indicating that higher limit was given to less people, which makes sense


#### AGE
```{r}
create_histogram(credit_card_data, credit_card_data$AGE, "AGE", 60)
```

Age is pretty much normally distributed, without much people below 20, which makes sense as credit card is not given to people below 18
The mean and median age of credit card customers is about 35 years
No. of customers peaks in the range of 27 - 31 years



### Creating bar plots for factor variables SEX, MARRIAGE, EDUCATION and DEFAULT PAYMENT

#### SEX
```{r}
create_bar_plot(credit_card_data, credit_card_data$SEX, "SEX")
```

60 % of the customers are females compared to 40% males


#### EDUCATION
```{r}
create_bar_plot(credit_card_data, credit_card_data$EDUCATION, "EDUCATION")
```

We can see that about 47% customers have attended university, and about 35% have attended graduate school


#### MARRIAGE
```{r}
create_bar_plot(credit_card_data, credit_card_data$MARRIAGE, "MARRIAGE")
```

There are more customers who are Single as opposed those who are Married


#### DEFAULT PAYMENT NEXT MONTH
```{r}
create_bar_plot(credit_card_data, credit_card_data$default.payment.next.month, "DEFAULT PAYMENT NEXT MONTH")
```

About 21% of the customers defaulted


### Comparing DEFAULT PAYMENT NEXT MONTH with respect to SEX, MARRIAGE and EDUCATION

#### SEX Vs DEFAULT PAYMENT NEXT MONTH
```{r}
create_stacked_bar_plot(credit_card_data, credit_card_data$SEX, "SEX", credit_card_data$default.payment.next.month, "DEFAULT PAYMENT NEXT MONTH")
```

We see that 25% of male customers defaulted, compared to 20% female customers 


#### EDUCATION Vs DEFAULT PAYMENT NEXT MONTH
```{r}
create_stacked_bar_plot(credit_card_data, credit_card_data$EDUCATION, "EDUCATION", credit_card_data$default.payment.next.month, "DEFAULT PAYMENT NEXT MONTH")
```

We see that about 20 % of those who attended Graduate School Defaulted
About 22% of University customers defaulted and about 25 % of customers who just attended high school defaulted


#### MARRIAGE Vs DEFAULT PAYMENT NEXT MONTH
```{r}
create_stacked_bar_plot(credit_card_data, credit_card_data$MARRIAGE, "MARRIAGE", credit_card_data$default.payment.next.month, "DEFAULT PAYMENT NEXT MONTH")
```

About 24 % of married customers defaulted as opposed to 20 % single customers


### Creating Feature Density Plots 

Creating Feature Density Plots LIMIT_BAL, AGE, BAL_AMTs and PAY_AMTs to see which features have discriminating power.
Plot ScatterPlot of features to determine if there is any pattern overlayed Density Plots
Interpretation: Density plots for default payment Yes and No almost overlap for Limit_bal, clear non-overlapping cases have more discriminating power

```{r}
plot_type <- "density"
create_feature_plot(numeric_var_data, credit_card_data$default.payment.next.month, plot_type)
```

#### LIMIT_BAL
We can see that the density plots of defaulters and non-defaulters is fairly different with respect to balance limit 

#### AGE
We can see that the density plots of defaulters and non-defaulters is slightly different with respect to balance limit 

#### BILL AMTs
Through visual inspection we can infer that balance amounts do not have differentiating power

#### PAY AMTs
Through visual inspection we can infer that payment amounts do not have differentiating power


### Outlier Detection

#### Creating box plots to do visual inspection about outliers
```{r}
plot_type <- "box"
create_feature_plot(numeric_var_data, credit_card_data$default.payment.next.month, plot_type)
```


### Feature Selection

#### Determining association between the target variable and the other categorical variables using Chi-squared Test for Independence
```{r}
lapply(factor_var_data, function(x) chi.squared(x~., factor_var_data))
```

We can see that demographic details like SEX, EDUCATION and MARRIAGE are not very good variables for differentiating between defaulters and non-defaulters
Previous payment statuses are well correlated with the target variable, with the most recent one being the most correlated
Therefore, we will exclude these variables from modeling

```{r}
credit_card_data$SEX <- NULL
credit_card_data$EDUCATION <- NULL
credit_card_data$MARRIAGE <- NULL
```

Also we can see that PAY_0 is strongly correlated with PAY_2 and PAY_3
PAY_2 is strongly correlated with PAY_3
PAY_3 is strongly correlated with PAY_4 and PAY_5
PAY_4 is strongly correlated with PAY_5 and PAY_6
PAY_5 is strongly correlated with PAY_3 and PAY_4 and PAY_6
PAY_6 is strongly correlated with PAY_4 and PAY_5

Thus, in effect all payment statuses from PAY_0 to PAY_6 are strongly correlated with each other
To avoid problems of multicollinearity, we will just retain the most recent status PAY_0
```{r}
credit_card_data$PAY_2 <- NULL
credit_card_data$PAY_3 <- NULL
credit_card_data$PAY_4 <- NULL
credit_card_data$PAY_5 <- NULL
credit_card_data$PAY_6 <- NULL
```

#### Determining correlation between numerical variables
```{r}
corr_matrix <- cor(numeric_var_data)
round(corr_matrix, 2)
```

We can see that all Billed Amounts variables i.e. BILL_AMT1 through BILL_AMT6 are highly correlated to each other
Therefore, to avoid multi-collinearity problem, we will only retain BILL_AMT1 and get rid of the other BilledAmount variables

```{r}
credit_card_data$BILL_AMT2 <- NULL
credit_card_data$BILL_AMT3 <- NULL
credit_card_data$BILL_AMT4 <- NULL
credit_card_data$BILL_AMT5 <- NULL
credit_card_data$BILL_AMT6 <- NULL
```

#### Automatic Feature Selection using Mean Decrease in Gini
```{r}
resampled_balanced_credit_data <- SMOTE(default.payment.next.month ~ ., credit_card_data, perc.over = 100, perc.under = 200)
fs_model <- randomForest(default.payment.next.month~., data=resampled_balanced_credit_data, importance=TRUE)

imp <- as.data.frame(randomForest::importance(fs_model))
imp <- data.frame(MeanDecreaseGini = imp$MeanDecreaseGini,
                  names   = rownames(imp))
imp[order(imp$MeanDecreaseGini,decreasing = T),]
```

```{r}
randomForest::varImpPlot(fs_model)
```

The variable whose removal from the model leads most decrease in the GINI value is the most differentiating and thus the most important variable

Now we will select only the variables which lead to a substantial decrease in GINI value relative to the other variables

We choose 
PAY_0 - Repayment status in the last payment cycle
BILL_AMT1 - Amount of bill statement in the last payment cycle (NT dollar)
AGE - Age of the debtor
PAY_AMT1 - Amount paid in the last payment cycle
PAY_AMT2 - Amount paid in the 2nd last payment cycle
LIMIT_BAL - Amount of given credit in NT dollars
PAY_AMT3 - Amount paid in the 3rd last payment cycle
PAY_AMT6 - Amount paid in the 6th last payment cycle
PAY_AMT5 - Amount paid in the 5th last payment cycle
PAY_AMT4 - Amount paid in the 4th last payment cycle

The variables selected by the algorithm also make business sense, as payment behavior in recent past along with the credit limit and age of the debtor should give a good idea about likelihood of default

```{r}
features_to_select <- c("PAY_0", "BILL_AMT1", "AGE", "PAY_AMT1", "PAY_AMT2", "PAY_AMT3", "PAY_AMT4", "PAY_AMT5", "PAY_AMT6", "LIMIT_BAL","default.payment.next.month")
```
  
  
## Data Partitioning: Creating training, validation and test data sets
```{r}
set.seed(430)
trn_idx <- createDataPartition(credit_card_data$default.payment.next.month, p = 0.8, list = FALSE)
X_train <- credit_card_data[trn_idx, which(names(credit_card_data) %in% features_to_select)]
X_valid_test <- credit_card_data[-trn_idx, which(names(credit_card_data) %in% features_to_select)]

valid_idx <- createDataPartition(X_valid_test$default.payment.next.month, p = 0.5, list = FALSE)
X_valid <- X_valid_test[valid_idx, ]
X_test <- X_valid_test[-valid_idx, ]
```

Since the data set is unbalanced, with Yes: No being 1:4
This can be an issue as the 'No' class will dominate the outcome of classification
Therefore we will create a new data set using SMOTE such that we get a balanced data set by oversampling 'Yes' observations and undersampling 'No' observations

```{r}
prop.table(table(X_train$default.payment.next.month))
```

```{r}
X_train <- SMOTE(default.payment.next.month ~ ., X_train, perc.over = 100, perc.under = 200)

prop.table(table(X_train$default.payment.next.month))
```


## Modeling

### Defining a 10-folds cross-validation scheme
```{r}
folds <- 10
cvIndex <- createFolds(factor(X_train$default.payment.next.month), folds, returnTrain = T)
control <- trainControl(index = cvIndex, method = "cv", number = folds, search = "random", allowParallel = T, classProbs = T, summaryFunction = f1)
```

### Metric for Optimization

Since our objective is to maximize the detection of Defaulters, we want our classifiers
to be sensitive to the positive class, at the same time trying to minimize the cases of false positives
i.e. labeling someone as Defaulter when they are not
Therefore, we will use F1-score as the metric, which is mathematically the harmonic mean of
precision and recall
```{r}
metric <- "F1"
```

### Logistic Regression

#### Training the Model
```{r}
method <- "glm"

model <- train(
  form = default.payment.next.month ~ .,
  data = X_train,
  trControl = control,
  method = method,
  metric = metric
)

summary(model)
```

#### Model Evaluation

#### Training
```{r}
data <- X_train
```

#### Kolmogorov-Smirnov (KS) Statistic

We will calculate KS statistic on the training data to get the best threshold at which we get the maximum separation between the two classes of the target variable

```{r}
ks_statistic <- KS(predict(model, newdata=data), data$default.payment.next.month)

threshold <- ks_statistic
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model, threshold)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

#### Validation
```{r}
data <- X_valid
```

#### Kolmogorov-Smirnov (KS) Statistic
```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model, threshold)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

#### Test
```{r}
data <- X_test
```

#### Kolmogorov-Smirnov (KS) Statistic
```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model, threshold)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```


### XGBoost Classifier

#### Training the Model
```{r}
method <- "xgbTree"

model <- train(
  form = default.payment.next.month ~ .,
  data = X_train,
  trControl = control,
  method = method,
  metric = metric
)

summary(model)
```

#### Best parameters for the model
```{r}
model$bestTune
```

#### Training the model with the best parameters
```{r}
final_grid <- expand.grid(
                nrounds = model$bestTune[, 'nrounds'],
                eta = model$bestTune[, 'eta'],
                max_depth = model$bestTune[, 'max_depth'], 
                gamma = model$bestTune[, 'gamma'],
                colsample_bytree = model$bestTune[, 'colsample_bytree'],
                min_child_weight = model$bestTune[, 'min_child_weight'],
                subsample = model$bestTune[, 'subsample']
              )

model <- train(
                form = default.payment.next.month ~ .,
                data = X_train,
                trControl = control,
                method = method,
                tuneGrid = final_grid,
                metric = metric
              )
```

#### Model Evaluation

#### Training
```{r}
data <- X_train
```

#### Kolmogorov-Smirnov (KS) Statistic
```{r}
ks_statistic <- KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

#### Validation
```{r}
data <- X_valid
```

#### Kolmogorov-Smirnov (KS) Statistic
```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model, threshold)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

#### Test
```{r}
data <- X_test
```

#### Kolmogorov-Smirnov (KS) Statistic
```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model, threshold)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

### Naive-Bayes Classifier

#### Training the Model
```{r}
method <- "naive_bayes"

model <- train(
  form = default.payment.next.month ~ .,
  data = X_train,
  trControl = control,
  method = method,
  metric = metric
)

summary(model)
```

#### Best parameters for the model
```{r}
model$bestTune
```

#### Training the model with the best parameters
```{r}
final_grid <- expand.grid(
  laplace = model$bestTune[, 'laplace'],
  usekernel = model$bestTune[, 'usekernel'],
  adjust = model$bestTune[, 'adjust'] 
)

model <- train(
                form = default.payment.next.month ~ .,
                data = X_train,
                trControl = control,
                method = method,
                tuneGrid = final_grid,
                metric = metric
              )
```

#### Model Evaluation

#### Training
```{r}
data <- X_train
```

#### Kolmogorov-Smirnov (KS) Statistic

```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

#### Validation
```{r}
data <- X_valid
```

#### Kolmogorov-Smirnov (KS) Statistic
```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

#### Test
```{r}
data <- X_test
```

#### Kolmogorov-Smirnov (KS) Statistic
```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

### Random Forest Classifier

#### Training the Model
```{r}
method <- "RRF"

model <- train(
  form = default.payment.next.month ~ .,
  data = X_train,
  trControl = control,
  method = method,
  metric = metric
)

summary(model)
```

#### Best parameters for the model
```{r}
model$bestTune
```

#### Training the model with the best parameters
```{r}
final_grid <- expand.grid(
  mtry = model$bestTune[, 'mtry'],
  coefReg = model$bestTune[, 'coefReg'],
  coefImp = model$bestTune[, 'coefImp']
)

model <- train(
                form = default.payment.next.month ~ .,
                data = X_train,
                trControl = control,
                method = method,
                tuneGrid = final_grid,
                metric = metric
              )
```

#### Model Evaluation

#### Training
```{r}
data <- X_train
```

#### Kolmogorov-Smirnov (KS) Statistic

```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model, threshold)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

#### Validation
```{r}
data <- X_valid
```

#### Kolmogorov-Smirnov (KS) Statistic
```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```

#### Test
```{r}
data <- X_test
```

#### Kolmogorov-Smirnov (KS) Statistic
```{r}
KS(predict(model, newdata=data), data$default.payment.next.month)
```

##### Confusion Matrix
```{r}
evaluate_confusion_matrix(data, model)
```

##### ROC-AUC
```{r}
evaluate_roc_auc(data, model)
```